{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real NVAE Architecture Experiment on CIFAR-10\n",
    "\n",
    "This notebook implements the training and evaluation pipeline for the **Real NVAE Architecture** on CIFAR-10.\n",
    "\n",
    "Unlike the previous simplified experiments, this version implements:\n",
    "- **Deep Hierarchical Latent Space**: Multiple groups of latent variables per scale.\n",
    "- **Residual Parameterization**: Posterior distributions are learned as residuals to the prior.\n",
    "- **Cell-Based Architecture**: Encoder and Decoder built from repeating Residual Cells.\n",
    "- **Separable Convolutions**: Efficient depthwise separable convolutions throughout.\n",
    "\n",
    "This configuration is much closer to the official paper implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Google Colab Setup\n",
    "Mount Drive and clone the repository (Fresh Copy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "REPO_PATH = '/content/drive/MyDrive/Generative-Modeling-on-CIFAR-10'\n",
    "REPO_URL = \"https://github.com/konstantine25b/Generative-Modeling-on-CIFAR-10.git\"\n",
    "\n",
    "# 1. Delete repo if it already exists (Ensure fresh code)\n",
    "if os.path.exists(REPO_PATH):\n",
    "    print(f\"Deleting existing repository at {REPO_PATH}...\")\n",
    "    shutil.rmtree(REPO_PATH)\n",
    "\n",
    "# 2. Clone repository\n",
    "os.chdir('/content/drive/MyDrive')\n",
    "print(f\"Cloning repository to {REPO_PATH}...\")\n",
    "!git clone {REPO_URL}\n",
    "\n",
    "# 3. Enter the repository\n",
    "os.chdir(REPO_PATH)\n",
    "print(f\"Current working directory: {os.getcwd()}\")\n",
    "\n",
    "# 4. Add source code to Python path\n",
    "sys.path.append(os.path.join(REPO_PATH, 'src'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. GitHub Configuration (Optional)\n",
    "Configure this if you want to push results back to the repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GitHub Configuration & Setup\n",
    "import os\n",
    "\n",
    "try:\n",
    "    # 1. Configure Git\n",
    "    user_name = \"konstantine25b\"\n",
    "    mail = \"konstantine25b@gmail.com\"\n",
    "\n",
    "    # --- IMPORTANT: PASTE YOUR TOKEN BELOW ---\n",
    "    my_token = \"YOUR_TOKEN_HERE\"\n",
    "\n",
    "    if my_token == \"YOUR_TOKEN_HERE\":\n",
    "        print(\"⚠️ PLEASE UPDATE 'my_token' in the code cell with your actual GitHub token to enable pushing.\")\n",
    "\n",
    "    repo_url = f\"https://{my_token}@github.com/konstantine25b/Generative-Modeling-on-CIFAR-10.git\"\n",
    "\n",
    "    !git config --global user.name \"{user_name}\"\n",
    "    !git config --global user.email \"{mail}\"\n",
    "\n",
    "    # 2. Set Remote URL\n",
    "    if os.path.isdir(\".git\") and my_token != \"YOUR_TOKEN_HERE\":\n",
    "        !git remote set-url origin \"{repo_url}\"\n",
    "        print(\"Git configured successfully for pushing.\")\n",
    "    else:\n",
    "        print(\"Skipping remote setup (either not a git repo or token not set).\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error setting up GitHub: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt\n",
    "!pip install wandb -q\n",
    "\n",
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Setup Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from src.utils.data_loader import get_cifar10_loaders\n",
    "from src.vae.train import train_vae\n",
    "from src.vae.sampling import generate_samples, save_sample_grid\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torchvision\n",
    "import wandb\n",
    "\n",
    "# Configuration for Real NVAE\n",
    "config = {\n",
    "    'epochs': 100,              # Training for longer due to increased depth\n",
    "    'batch_size': 64,           # Reduced batch size to fit deeper model in memory\n",
    "    'lr': 1e-3,                 # Slightly higher LR for AdamAX/AdamW with deep models often helps, but sticking to 1e-3 is safe\n",
    "    'hidden_dim': 64,           # Base channel width\n",
    "    'latent_dim': 20,           # Latent dimension per group\n",
    "    'num_scales': 3,            # 32x32, 16x16, 8x8\n",
    "    'warmup_epochs': 10,         # KL Warmup\n",
    "    'weight_decay': 3e-4,\n",
    "    'use_wandb': True,\n",
    "    'run_name': 'nvae_real_arch',\n",
    "    'model_save_dir': '/content/drive/MyDrive/Generative-Modeling-on-CIFAR-10-Checkpoints/nvae_real',\n",
    "    'results_dir': 'results/'\n",
    "}\n",
    "\n",
    "# Create directories\n",
    "os.makedirs(config['model_save_dir'], exist_ok=True)\n",
    "os.makedirs(config['results_dir'], exist_ok=True)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"Checkpoints will be saved to: {config['model_save_dir']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, val_loader, test_loader = get_cifar10_loaders(\n",
    "    data_dir='./data',\n",
    "    batch_size=config['batch_size']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Start Training\n",
    "\n",
    "This will train the deep NVAE model. Expect this to take significantly longer per epoch than the simplified version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start Training\n",
    "train_vae(config, train_loader, val_loader, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Evaluation\n",
    "\n",
    "After training, we evaluate using Importance Weighted Sampling (IWELBO) for a tight bound on the log-likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model\n",
    "from src.vae.model import NVAE\n",
    "from src.vae.train import evaluate_with_importance_sampling, evaluate\n",
    "\n",
    "best_model = NVAE(\n",
    "    hidden_dim=config['hidden_dim'], \n",
    "    latent_dim=config['latent_dim']\n",
    ").to(device)\n",
    "\n",
    "checkpoint_path = os.path.join(config['model_save_dir'], 'nvae_best.pth')\n",
    "checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "\n",
    "if isinstance(checkpoint, dict) and 'model_state_dict' in checkpoint:\n",
    "    best_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "else:\n",
    "    best_model.load_state_dict(checkpoint)\n",
    "\n",
    "print(f\"Loaded best model from {checkpoint_path}\")\n",
    "\n",
    "# Standard Evaluation\n",
    "loss, bpd = evaluate(best_model, test_loader, device)\n",
    "print(f\"Standard Test Set Results (ELBO) -> Loss: {loss:.4f} | BPD: {bpd:.4f}\")\n",
    "\n",
    "# Importance Weighted Evaluation\n",
    "# k=100 provides a good balance between speed and accuracy for debugging\n",
    "# Paper uses k=1000\n",
    "iw_loss, iw_bpd = evaluate_with_importance_sampling(best_model, test_loader, device, k=100)\n",
    "print(f\"Importance Weighted Results (k=100) -> Loss: {iw_loss:.4f} | BPD: {iw_bpd:.4f}\")\n",
    "\n",
    "# Log final results to WandB\n",
    "if config['use_wandb']:\n",
    "    wandb.log({\n",
    "        \"test/iw_loss\": iw_loss,\n",
    "        \"test/iw_bpd\": iw_bpd\n",
    "    })\n",
    "    print(\"Logged test results to WandB.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
